{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Классификация изображений с помощью сверточных нейронных сетей**\n",
    "\n",
    "В данном задании Вам необходимо разработать архитектуру сверточной ИНС, обеспечивающую наибольшую точность при ограничении на количество операций (FLOPs <= 0.707e6).\n",
    "Заготовка кода для выполнения задания приведена выше. Вашей задачей будет заполнить пропущеные места, которые отмечены ключевым словом *None*.\n",
    "Необходимая точность (accuracy) сети на датасете CIFAR100 - 30%\n",
    "Желаемая точность (accuracy) сети на датасете CIFAR100 - 45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:05:07.688278Z",
     "iopub.status.busy": "2023-01-24T18:05:07.687913Z",
     "iopub.status.idle": "2023-01-24T18:05:17.213064Z",
     "shell.execute_reply": "2023-01-24T18:05:17.211842Z",
     "shell.execute_reply.started": "2023-01-24T18:05:07.688245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-flops in /opt/conda/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: tensorflow<3.0,>=2.2 in /opt/conda/lib/python3.7/site-packages (from keras-flops) (2.6.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.51.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.20.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.15.0)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (5.0)\n",
      "Requirement already satisfied: typing-extensions<3.11,>=3.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.10.0.2)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12.1)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.37.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.19.5)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.1.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow<3.0,>=2.2->keras-flops) (1.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (59.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.6.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.13.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install keras-flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-24T18:05:55.698665Z",
     "iopub.status.busy": "2023-01-24T18:05:55.698234Z",
     "iopub.status.idle": "2023-01-24T18:05:55.708760Z",
     "shell.execute_reply": "2023-01-24T18:05:55.707702Z",
     "shell.execute_reply.started": "2023-01-24T18:05:55.698625Z"
    }
   },
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras_flops import get_flops\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:05:59.113940Z",
     "iopub.status.busy": "2023-01-24T18:05:59.113575Z",
     "iopub.status.idle": "2023-01-24T18:05:59.119276Z",
     "shell.execute_reply": "2023-01-24T18:05:59.117908Z",
     "shell.execute_reply.started": "2023-01-24T18:05:59.113908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Глобальные константы\n",
    "CLASSES       = 100\n",
    "BATCH_SIZE    = 128\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:06:02.285442Z",
     "iopub.status.busy": "2023-01-24T18:06:02.284342Z",
     "iopub.status.idle": "2023-01-24T18:06:03.439409Z",
     "shell.execute_reply": "2023-01-24T18:06:03.438377Z",
     "shell.execute_reply.started": "2023-01-24T18:06:02.285394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Выполните загрузку модели\n",
    "(X_train, y_train), (X_val, y_val) = tf.keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:06:10.263355Z",
     "iopub.status.busy": "2023-01-24T18:06:10.262994Z",
     "iopub.status.idle": "2023-01-24T18:06:10.278518Z",
     "shell.execute_reply": "2023-01-24T18:06:10.277510Z",
     "shell.execute_reply.started": "2023-01-24T18:06:10.263324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Преобразуйте метки классов в one_hot формат\n",
    "y_train = np_utils.to_categorical(y_train, CLASSES)\n",
    "y_val = np_utils.to_categorical(y_val, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:06:13.119209Z",
     "iopub.status.busy": "2023-01-24T18:06:13.118848Z",
     "iopub.status.idle": "2023-01-24T18:06:13.126158Z",
     "shell.execute_reply": "2023-01-24T18:06:13.125017Z",
     "shell.execute_reply.started": "2023-01-24T18:06:13.119174Z"
    }
   },
   "outputs": [],
   "source": [
    "# убедитесь, что данная ячейка выполняется без ошибок\n",
    "assert X_train.shape == (50000, 32, 32, 3)\n",
    "assert X_val.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 100)\n",
    "assert y_val.shape == (10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:56:22.975775Z",
     "iopub.status.busy": "2023-01-24T18:56:22.974754Z",
     "iopub.status.idle": "2023-01-24T18:56:23.018686Z",
     "shell.execute_reply": "2023-01-24T18:56:23.017798Z",
     "shell.execute_reply.started": "2023-01-24T18:56:22.975728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Задайте архитектуру модели\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=[32,32,3]),\n",
    "    tf.keras.layers.Conv2D(8, 3, padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(9, strides=9),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(CLASSES),\n",
    "    tf.keras.layers.Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:56:26.045268Z",
     "iopub.status.busy": "2023-01-24T18:56:26.044901Z",
     "iopub.status.idle": "2023-01-24T18:56:26.110423Z",
     "shell.execute_reply": "2023-01-24T18:56:26.109411Z",
     "shell.execute_reply.started": "2023-01-24T18:56:26.045236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/471.58k flops)\n",
      "  sequential_9/conv2d_9/Conv2D (442.37k/442.37k flops)\n",
      "  sequential_9/dense_9/MatMul (14.40k/14.40k flops)\n",
      "  sequential_9/conv2d_9/BiasAdd (8.19k/8.19k flops)\n",
      "  sequential_9/max_pooling2d_9/MaxPool (5.83k/5.83k flops)\n",
      "  sequential_9/activation_19/Softmax (500/500 flops)\n",
      "  sequential_9/batch_normalization_9/FusedBatchNormV3 (192/192 flops)\n",
      "  sequential_9/dense_9/BiasAdd (100/100 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "FLOPs: 0.4716e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 18:56:26.072499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 18:56:26.072910: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-01-24 18:56:26.073025: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2023-01-24 18:56:26.073450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 18:56:26.073857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 18:56:26.074185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 18:56:26.074569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 18:56:26.074919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-24 18:56:26.075169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2023-01-24 18:56:26.076311: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# вычисление количества операций\n",
    "flops = get_flops(model, batch_size=1)\n",
    "print(f\"FLOPs: {(flops / 1e6):.4f}e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:56:36.428393Z",
     "iopub.status.busy": "2023-01-24T18:56:36.428016Z",
     "iopub.status.idle": "2023-01-24T18:56:36.434990Z",
     "shell.execute_reply": "2023-01-24T18:56:36.433845Z",
     "shell.execute_reply.started": "2023-01-24T18:56:36.428358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 8)         224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 3, 3, 8)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 3, 3, 8)           32        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 3, 3, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               7300      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 7,556\n",
      "Trainable params: 7,540\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# вывод краткой информации о модели\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:57:09.169134Z",
     "iopub.status.busy": "2023-01-24T18:57:09.168762Z",
     "iopub.status.idle": "2023-01-24T18:57:09.181929Z",
     "shell.execute_reply": "2023-01-24T18:57:09.180916Z",
     "shell.execute_reply.started": "2023-01-24T18:57:09.169100Z"
    }
   },
   "outputs": [],
   "source": [
    "# параметры данной ячейки могут быть изменены для получения более высокой точности\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adamax(\n",
    "        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=LEARNING_RATE, decay_steps=10000, decay_rate=0.3)\n",
    "    ),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:57:12.052889Z",
     "iopub.status.busy": "2023-01-24T18:57:12.052501Z",
     "iopub.status.idle": "2023-01-24T19:03:08.659658Z",
     "shell.execute_reply": "2023-01-24T19:03:08.658594Z",
     "shell.execute_reply.started": "2023-01-24T18:57:12.052855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 3.9398 - accuracy: 0.1082 - val_loss: 3.6931 - val_accuracy: 0.1492\n",
      "Epoch 2/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 3.4750 - accuracy: 0.1850 - val_loss: 3.4819 - val_accuracy: 0.1819\n",
      "Epoch 3/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.3188 - accuracy: 0.2103 - val_loss: 3.3557 - val_accuracy: 0.2089\n",
      "Epoch 4/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.2412 - accuracy: 0.2267 - val_loss: 3.3446 - val_accuracy: 0.2183\n",
      "Epoch 5/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.1931 - accuracy: 0.2378 - val_loss: 3.3044 - val_accuracy: 0.2230\n",
      "Epoch 6/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.1563 - accuracy: 0.2458 - val_loss: 3.2498 - val_accuracy: 0.2302\n",
      "Epoch 7/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 3.1264 - accuracy: 0.2512 - val_loss: 3.2516 - val_accuracy: 0.2291\n",
      "Epoch 8/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 3.1047 - accuracy: 0.2570 - val_loss: 3.2495 - val_accuracy: 0.2356\n",
      "Epoch 9/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 3.0811 - accuracy: 0.2615 - val_loss: 3.1939 - val_accuracy: 0.2403\n",
      "Epoch 10/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 3.0675 - accuracy: 0.2652 - val_loss: 3.1743 - val_accuracy: 0.2484\n",
      "Epoch 11/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.0527 - accuracy: 0.2670 - val_loss: 3.1449 - val_accuracy: 0.2556\n",
      "Epoch 12/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.0395 - accuracy: 0.2712 - val_loss: 3.2517 - val_accuracy: 0.2349\n",
      "Epoch 13/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.0278 - accuracy: 0.2736 - val_loss: 3.1515 - val_accuracy: 0.2479\n",
      "Epoch 14/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.0201 - accuracy: 0.2747 - val_loss: 3.1929 - val_accuracy: 0.2425\n",
      "Epoch 15/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.0102 - accuracy: 0.2755 - val_loss: 3.1166 - val_accuracy: 0.2596\n",
      "Epoch 16/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 3.0007 - accuracy: 0.2766 - val_loss: 3.1572 - val_accuracy: 0.2460\n",
      "Epoch 17/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9940 - accuracy: 0.2785 - val_loss: 3.1264 - val_accuracy: 0.2600\n",
      "Epoch 18/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.9877 - accuracy: 0.2799 - val_loss: 3.1331 - val_accuracy: 0.2554\n",
      "Epoch 19/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9813 - accuracy: 0.2799 - val_loss: 3.1339 - val_accuracy: 0.2544\n",
      "Epoch 20/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9765 - accuracy: 0.2833 - val_loss: 3.0953 - val_accuracy: 0.2633\n",
      "Epoch 21/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9722 - accuracy: 0.2838 - val_loss: 3.0976 - val_accuracy: 0.2574\n",
      "Epoch 22/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9651 - accuracy: 0.2849 - val_loss: 3.0959 - val_accuracy: 0.2601\n",
      "Epoch 23/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9604 - accuracy: 0.2863 - val_loss: 3.0847 - val_accuracy: 0.2636\n",
      "Epoch 24/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9558 - accuracy: 0.2860 - val_loss: 3.1311 - val_accuracy: 0.2557\n",
      "Epoch 25/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9521 - accuracy: 0.2871 - val_loss: 3.1075 - val_accuracy: 0.2619\n",
      "Epoch 26/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.9492 - accuracy: 0.2883 - val_loss: 3.0773 - val_accuracy: 0.2676\n",
      "Epoch 27/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9466 - accuracy: 0.2884 - val_loss: 3.0803 - val_accuracy: 0.2633\n",
      "Epoch 28/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9425 - accuracy: 0.2883 - val_loss: 3.1394 - val_accuracy: 0.2556\n",
      "Epoch 29/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9415 - accuracy: 0.2897 - val_loss: 3.0863 - val_accuracy: 0.2585\n",
      "Epoch 30/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9387 - accuracy: 0.2905 - val_loss: 3.0727 - val_accuracy: 0.2627\n",
      "Epoch 31/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.9348 - accuracy: 0.2895 - val_loss: 3.0646 - val_accuracy: 0.2668\n",
      "Epoch 32/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.9329 - accuracy: 0.2908 - val_loss: 3.0919 - val_accuracy: 0.2582\n",
      "Epoch 33/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9318 - accuracy: 0.2914 - val_loss: 3.0585 - val_accuracy: 0.2670\n",
      "Epoch 34/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.9283 - accuracy: 0.2917 - val_loss: 3.0639 - val_accuracy: 0.2643\n",
      "Epoch 35/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9268 - accuracy: 0.2920 - val_loss: 3.0518 - val_accuracy: 0.2710\n",
      "Epoch 36/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9219 - accuracy: 0.2940 - val_loss: 3.0600 - val_accuracy: 0.2642\n",
      "Epoch 37/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9228 - accuracy: 0.2944 - val_loss: 3.0839 - val_accuracy: 0.2632\n",
      "Epoch 38/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9187 - accuracy: 0.2943 - val_loss: 3.0553 - val_accuracy: 0.2682\n",
      "Epoch 39/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9197 - accuracy: 0.2937 - val_loss: 3.0600 - val_accuracy: 0.2686\n",
      "Epoch 40/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9153 - accuracy: 0.2940 - val_loss: 3.0475 - val_accuracy: 0.2701\n",
      "Epoch 41/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9154 - accuracy: 0.2938 - val_loss: 3.0456 - val_accuracy: 0.2724\n",
      "Epoch 42/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.9135 - accuracy: 0.2941 - val_loss: 3.0701 - val_accuracy: 0.2672\n",
      "Epoch 43/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.9126 - accuracy: 0.2951 - val_loss: 3.0473 - val_accuracy: 0.2687\n",
      "Epoch 44/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9109 - accuracy: 0.2953 - val_loss: 3.0409 - val_accuracy: 0.2708\n",
      "Epoch 45/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9091 - accuracy: 0.2956 - val_loss: 3.0437 - val_accuracy: 0.2730\n",
      "Epoch 46/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9071 - accuracy: 0.2956 - val_loss: 3.0438 - val_accuracy: 0.2704\n",
      "Epoch 47/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9076 - accuracy: 0.2959 - val_loss: 3.0367 - val_accuracy: 0.2726\n",
      "Epoch 48/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9049 - accuracy: 0.2962 - val_loss: 3.0443 - val_accuracy: 0.2691\n",
      "Epoch 49/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9057 - accuracy: 0.2957 - val_loss: 3.0331 - val_accuracy: 0.2715\n",
      "Epoch 50/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.9053 - accuracy: 0.2965 - val_loss: 3.0421 - val_accuracy: 0.2702\n",
      "Epoch 51/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9042 - accuracy: 0.2966 - val_loss: 3.0384 - val_accuracy: 0.2699\n",
      "Epoch 52/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.9034 - accuracy: 0.2971 - val_loss: 3.0363 - val_accuracy: 0.2693\n",
      "Epoch 53/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9018 - accuracy: 0.2980 - val_loss: 3.0417 - val_accuracy: 0.2726\n",
      "Epoch 54/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.9010 - accuracy: 0.2975 - val_loss: 3.0380 - val_accuracy: 0.2682\n",
      "Epoch 55/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.9008 - accuracy: 0.2976 - val_loss: 3.0376 - val_accuracy: 0.2685\n",
      "Epoch 56/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8990 - accuracy: 0.2974 - val_loss: 3.0337 - val_accuracy: 0.2695\n",
      "Epoch 57/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8990 - accuracy: 0.2983 - val_loss: 3.0319 - val_accuracy: 0.2723\n",
      "Epoch 58/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8986 - accuracy: 0.2978 - val_loss: 3.0367 - val_accuracy: 0.2710\n",
      "Epoch 59/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8985 - accuracy: 0.2973 - val_loss: 3.0320 - val_accuracy: 0.2691\n",
      "Epoch 60/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8968 - accuracy: 0.2989 - val_loss: 3.0316 - val_accuracy: 0.2708\n",
      "Epoch 61/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8958 - accuracy: 0.2978 - val_loss: 3.0326 - val_accuracy: 0.2719\n",
      "Epoch 62/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8956 - accuracy: 0.2981 - val_loss: 3.0317 - val_accuracy: 0.2707\n",
      "Epoch 63/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8949 - accuracy: 0.2992 - val_loss: 3.0350 - val_accuracy: 0.2682\n",
      "Epoch 64/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8931 - accuracy: 0.2988 - val_loss: 3.0305 - val_accuracy: 0.2740\n",
      "Epoch 65/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8947 - accuracy: 0.2977 - val_loss: 3.0277 - val_accuracy: 0.2708\n",
      "Epoch 66/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8941 - accuracy: 0.2989 - val_loss: 3.0331 - val_accuracy: 0.2691\n",
      "Epoch 67/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8931 - accuracy: 0.2979 - val_loss: 3.0390 - val_accuracy: 0.2666\n",
      "Epoch 68/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8936 - accuracy: 0.2992 - val_loss: 3.0261 - val_accuracy: 0.2732\n",
      "Epoch 69/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8923 - accuracy: 0.2991 - val_loss: 3.0360 - val_accuracy: 0.2677\n",
      "Epoch 70/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8929 - accuracy: 0.2990 - val_loss: 3.0269 - val_accuracy: 0.2713\n",
      "Epoch 71/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8929 - accuracy: 0.2989 - val_loss: 3.0250 - val_accuracy: 0.2721\n",
      "Epoch 72/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8923 - accuracy: 0.2986 - val_loss: 3.0265 - val_accuracy: 0.2736\n",
      "Epoch 73/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8904 - accuracy: 0.2992 - val_loss: 3.0275 - val_accuracy: 0.2724\n",
      "Epoch 74/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8927 - accuracy: 0.2990 - val_loss: 3.0258 - val_accuracy: 0.2705\n",
      "Epoch 75/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8913 - accuracy: 0.2997 - val_loss: 3.0264 - val_accuracy: 0.2753\n",
      "Epoch 76/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8920 - accuracy: 0.2993 - val_loss: 3.0271 - val_accuracy: 0.2725\n",
      "Epoch 77/256\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.8905 - accuracy: 0.2996 - val_loss: 3.0251 - val_accuracy: 0.2725\n",
      "Epoch 78/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8908 - accuracy: 0.3001 - val_loss: 3.0251 - val_accuracy: 0.2736\n",
      "Epoch 79/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8900 - accuracy: 0.2994 - val_loss: 3.0247 - val_accuracy: 0.2741\n",
      "Epoch 80/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8903 - accuracy: 0.2985 - val_loss: 3.0260 - val_accuracy: 0.2716\n",
      "Epoch 81/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8893 - accuracy: 0.2992 - val_loss: 3.0258 - val_accuracy: 0.2745\n",
      "Epoch 82/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8908 - accuracy: 0.2988 - val_loss: 3.0233 - val_accuracy: 0.2733\n",
      "Epoch 83/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8881 - accuracy: 0.3004 - val_loss: 3.0256 - val_accuracy: 0.2704\n",
      "Epoch 84/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8887 - accuracy: 0.2984 - val_loss: 3.0255 - val_accuracy: 0.2724\n",
      "Epoch 85/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8898 - accuracy: 0.2989 - val_loss: 3.0250 - val_accuracy: 0.2728\n",
      "Epoch 86/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8890 - accuracy: 0.2995 - val_loss: 3.0239 - val_accuracy: 0.2737\n",
      "Epoch 87/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8892 - accuracy: 0.2999 - val_loss: 3.0235 - val_accuracy: 0.2727\n",
      "Epoch 88/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8890 - accuracy: 0.2996 - val_loss: 3.0229 - val_accuracy: 0.2731\n",
      "Epoch 89/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8875 - accuracy: 0.3009 - val_loss: 3.0242 - val_accuracy: 0.2727\n",
      "Epoch 90/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8886 - accuracy: 0.2994 - val_loss: 3.0234 - val_accuracy: 0.2743\n",
      "Epoch 91/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8876 - accuracy: 0.3011 - val_loss: 3.0229 - val_accuracy: 0.2736\n",
      "Epoch 92/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8881 - accuracy: 0.2994 - val_loss: 3.0272 - val_accuracy: 0.2711\n",
      "Epoch 93/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8873 - accuracy: 0.3008 - val_loss: 3.0230 - val_accuracy: 0.2736\n",
      "Epoch 94/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8871 - accuracy: 0.2991 - val_loss: 3.0237 - val_accuracy: 0.2729\n",
      "Epoch 95/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8873 - accuracy: 0.3002 - val_loss: 3.0226 - val_accuracy: 0.2725\n",
      "Epoch 96/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8884 - accuracy: 0.2999 - val_loss: 3.0224 - val_accuracy: 0.2734\n",
      "Epoch 97/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8880 - accuracy: 0.3004 - val_loss: 3.0225 - val_accuracy: 0.2738\n",
      "Epoch 98/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8878 - accuracy: 0.3001 - val_loss: 3.0232 - val_accuracy: 0.2734\n",
      "Epoch 99/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8861 - accuracy: 0.3004 - val_loss: 3.0223 - val_accuracy: 0.2736\n",
      "Epoch 100/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8866 - accuracy: 0.3001 - val_loss: 3.0221 - val_accuracy: 0.2742\n",
      "Epoch 101/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8866 - accuracy: 0.2998 - val_loss: 3.0228 - val_accuracy: 0.2732\n",
      "Epoch 102/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8868 - accuracy: 0.3008 - val_loss: 3.0223 - val_accuracy: 0.2740\n",
      "Epoch 103/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8854 - accuracy: 0.2998 - val_loss: 3.0225 - val_accuracy: 0.2734\n",
      "Epoch 104/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8862 - accuracy: 0.2994 - val_loss: 3.0225 - val_accuracy: 0.2740\n",
      "Epoch 105/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8870 - accuracy: 0.3003 - val_loss: 3.0224 - val_accuracy: 0.2719\n",
      "Epoch 106/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8849 - accuracy: 0.3000 - val_loss: 3.0224 - val_accuracy: 0.2740\n",
      "Epoch 107/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8843 - accuracy: 0.3011 - val_loss: 3.0229 - val_accuracy: 0.2729\n",
      "Epoch 108/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8874 - accuracy: 0.2998 - val_loss: 3.0233 - val_accuracy: 0.2732\n",
      "Epoch 109/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8865 - accuracy: 0.3002 - val_loss: 3.0222 - val_accuracy: 0.2739\n",
      "Epoch 110/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8858 - accuracy: 0.3007 - val_loss: 3.0225 - val_accuracy: 0.2742\n",
      "Epoch 111/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8852 - accuracy: 0.3011 - val_loss: 3.0224 - val_accuracy: 0.2726\n",
      "Epoch 112/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8860 - accuracy: 0.3009 - val_loss: 3.0227 - val_accuracy: 0.2736\n",
      "Epoch 113/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8858 - accuracy: 0.3005 - val_loss: 3.0222 - val_accuracy: 0.2732\n",
      "Epoch 114/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8852 - accuracy: 0.3015 - val_loss: 3.0220 - val_accuracy: 0.2738\n",
      "Epoch 115/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8850 - accuracy: 0.3006 - val_loss: 3.0222 - val_accuracy: 0.2738\n",
      "Epoch 116/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8857 - accuracy: 0.3003 - val_loss: 3.0220 - val_accuracy: 0.2740\n",
      "Epoch 117/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8857 - accuracy: 0.3003 - val_loss: 3.0220 - val_accuracy: 0.2738\n",
      "Epoch 118/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8859 - accuracy: 0.3002 - val_loss: 3.0219 - val_accuracy: 0.2737\n",
      "Epoch 119/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8855 - accuracy: 0.2998 - val_loss: 3.0223 - val_accuracy: 0.2742\n",
      "Epoch 120/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8852 - accuracy: 0.3010 - val_loss: 3.0221 - val_accuracy: 0.2739\n",
      "Epoch 121/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8864 - accuracy: 0.3007 - val_loss: 3.0222 - val_accuracy: 0.2729\n",
      "Epoch 122/256\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.8870 - accuracy: 0.3007 - val_loss: 3.0220 - val_accuracy: 0.2731\n",
      "Epoch 123/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8844 - accuracy: 0.3017 - val_loss: 3.0220 - val_accuracy: 0.2742\n",
      "Epoch 124/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8857 - accuracy: 0.3004 - val_loss: 3.0222 - val_accuracy: 0.2736\n",
      "Epoch 125/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8860 - accuracy: 0.2999 - val_loss: 3.0220 - val_accuracy: 0.2736\n",
      "Epoch 126/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8857 - accuracy: 0.3008 - val_loss: 3.0217 - val_accuracy: 0.2740\n",
      "Epoch 127/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.2996 - val_loss: 3.0222 - val_accuracy: 0.2740\n",
      "Epoch 128/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8832 - accuracy: 0.3005 - val_loss: 3.0226 - val_accuracy: 0.2741\n",
      "Epoch 129/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8850 - accuracy: 0.3015 - val_loss: 3.0221 - val_accuracy: 0.2735\n",
      "Epoch 130/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8862 - accuracy: 0.3019 - val_loss: 3.0220 - val_accuracy: 0.2740\n",
      "Epoch 131/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8860 - accuracy: 0.3007 - val_loss: 3.0218 - val_accuracy: 0.2739\n",
      "Epoch 132/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8847 - accuracy: 0.3015 - val_loss: 3.0218 - val_accuracy: 0.2740\n",
      "Epoch 133/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8845 - accuracy: 0.3005 - val_loss: 3.0219 - val_accuracy: 0.2737\n",
      "Epoch 134/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.2998 - val_loss: 3.0219 - val_accuracy: 0.2736\n",
      "Epoch 135/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8840 - accuracy: 0.3021 - val_loss: 3.0218 - val_accuracy: 0.2738\n",
      "Epoch 136/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8847 - accuracy: 0.3010 - val_loss: 3.0221 - val_accuracy: 0.2735\n",
      "Epoch 137/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8842 - accuracy: 0.3001 - val_loss: 3.0223 - val_accuracy: 0.2732\n",
      "Epoch 138/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8856 - accuracy: 0.3001 - val_loss: 3.0221 - val_accuracy: 0.2742\n",
      "Epoch 139/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8850 - accuracy: 0.3006 - val_loss: 3.0220 - val_accuracy: 0.2738\n",
      "Epoch 140/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8857 - accuracy: 0.3009 - val_loss: 3.0219 - val_accuracy: 0.2744\n",
      "Epoch 141/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8845 - accuracy: 0.3005 - val_loss: 3.0218 - val_accuracy: 0.2738\n",
      "Epoch 142/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8851 - accuracy: 0.3007 - val_loss: 3.0220 - val_accuracy: 0.2739\n",
      "Epoch 143/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8847 - accuracy: 0.3001 - val_loss: 3.0217 - val_accuracy: 0.2744\n",
      "Epoch 144/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8844 - accuracy: 0.3001 - val_loss: 3.0217 - val_accuracy: 0.2741\n",
      "Epoch 145/256\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.8830 - accuracy: 0.2998 - val_loss: 3.0219 - val_accuracy: 0.2739\n",
      "Epoch 146/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8856 - accuracy: 0.3007 - val_loss: 3.0219 - val_accuracy: 0.2739\n",
      "Epoch 147/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8868 - accuracy: 0.3001 - val_loss: 3.0218 - val_accuracy: 0.2737\n",
      "Epoch 148/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8845 - accuracy: 0.2999 - val_loss: 3.0219 - val_accuracy: 0.2736\n",
      "Epoch 149/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8840 - accuracy: 0.3008 - val_loss: 3.0219 - val_accuracy: 0.2738\n",
      "Epoch 150/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8849 - accuracy: 0.3000 - val_loss: 3.0219 - val_accuracy: 0.2735\n",
      "Epoch 151/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8839 - accuracy: 0.2997 - val_loss: 3.0220 - val_accuracy: 0.2738\n",
      "Epoch 152/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8854 - accuracy: 0.3003 - val_loss: 3.0217 - val_accuracy: 0.2740\n",
      "Epoch 153/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8857 - accuracy: 0.2999 - val_loss: 3.0218 - val_accuracy: 0.2745\n",
      "Epoch 154/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8857 - accuracy: 0.3005 - val_loss: 3.0222 - val_accuracy: 0.2739\n",
      "Epoch 155/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8840 - accuracy: 0.3015 - val_loss: 3.0219 - val_accuracy: 0.2739\n",
      "Epoch 156/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8852 - accuracy: 0.3000 - val_loss: 3.0217 - val_accuracy: 0.2742\n",
      "Epoch 157/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8852 - accuracy: 0.3006 - val_loss: 3.0217 - val_accuracy: 0.2741\n",
      "Epoch 158/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8844 - accuracy: 0.3003 - val_loss: 3.0218 - val_accuracy: 0.2739\n",
      "Epoch 159/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.3006 - val_loss: 3.0220 - val_accuracy: 0.2743\n",
      "Epoch 160/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8840 - accuracy: 0.3001 - val_loss: 3.0220 - val_accuracy: 0.2739\n",
      "Epoch 161/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8854 - accuracy: 0.3004 - val_loss: 3.0220 - val_accuracy: 0.2739\n",
      "Epoch 162/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8853 - accuracy: 0.3007 - val_loss: 3.0222 - val_accuracy: 0.2742\n",
      "Epoch 163/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8853 - accuracy: 0.3001 - val_loss: 3.0218 - val_accuracy: 0.2738\n",
      "Epoch 164/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.3006 - val_loss: 3.0219 - val_accuracy: 0.2731\n",
      "Epoch 165/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8860 - accuracy: 0.2998 - val_loss: 3.0215 - val_accuracy: 0.2742\n",
      "Epoch 166/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8865 - accuracy: 0.3008 - val_loss: 3.0222 - val_accuracy: 0.2739\n",
      "Epoch 167/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8864 - accuracy: 0.3013 - val_loss: 3.0217 - val_accuracy: 0.2740\n",
      "Epoch 168/256\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.8855 - accuracy: 0.2997 - val_loss: 3.0219 - val_accuracy: 0.2736\n",
      "Epoch 169/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8840 - accuracy: 0.3011 - val_loss: 3.0216 - val_accuracy: 0.2734\n",
      "Epoch 170/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8849 - accuracy: 0.2998 - val_loss: 3.0219 - val_accuracy: 0.2733\n",
      "Epoch 171/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8844 - accuracy: 0.3010 - val_loss: 3.0218 - val_accuracy: 0.2736\n",
      "Epoch 172/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8853 - accuracy: 0.3020 - val_loss: 3.0220 - val_accuracy: 0.2738\n",
      "Epoch 173/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8852 - accuracy: 0.3012 - val_loss: 3.0217 - val_accuracy: 0.2738\n",
      "Epoch 174/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8846 - accuracy: 0.3019 - val_loss: 3.0221 - val_accuracy: 0.2739\n",
      "Epoch 175/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8842 - accuracy: 0.3011 - val_loss: 3.0216 - val_accuracy: 0.2740\n",
      "Epoch 176/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8834 - accuracy: 0.3015 - val_loss: 3.0217 - val_accuracy: 0.2744\n",
      "Epoch 177/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8867 - accuracy: 0.3004 - val_loss: 3.0218 - val_accuracy: 0.2738\n",
      "Epoch 178/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8864 - accuracy: 0.2999 - val_loss: 3.0221 - val_accuracy: 0.2733\n",
      "Epoch 179/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8847 - accuracy: 0.2995 - val_loss: 3.0221 - val_accuracy: 0.2736\n",
      "Epoch 180/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8853 - accuracy: 0.3014 - val_loss: 3.0218 - val_accuracy: 0.2739\n",
      "Epoch 181/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8846 - accuracy: 0.3000 - val_loss: 3.0216 - val_accuracy: 0.2738\n",
      "Epoch 182/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8859 - accuracy: 0.2999 - val_loss: 3.0218 - val_accuracy: 0.2739\n",
      "Epoch 183/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8843 - accuracy: 0.2997 - val_loss: 3.0218 - val_accuracy: 0.2735\n",
      "Epoch 184/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8847 - accuracy: 0.3007 - val_loss: 3.0219 - val_accuracy: 0.2741\n",
      "Epoch 185/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8845 - accuracy: 0.3009 - val_loss: 3.0220 - val_accuracy: 0.2744\n",
      "Epoch 186/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8848 - accuracy: 0.3011 - val_loss: 3.0218 - val_accuracy: 0.2737\n",
      "Epoch 187/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8844 - accuracy: 0.3009 - val_loss: 3.0218 - val_accuracy: 0.2738\n",
      "Epoch 188/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8838 - accuracy: 0.3011 - val_loss: 3.0219 - val_accuracy: 0.2742\n",
      "Epoch 189/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8858 - accuracy: 0.3006 - val_loss: 3.0220 - val_accuracy: 0.2737\n",
      "Epoch 190/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8847 - accuracy: 0.3011 - val_loss: 3.0219 - val_accuracy: 0.2741\n",
      "Epoch 191/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8852 - accuracy: 0.3007 - val_loss: 3.0217 - val_accuracy: 0.2746\n",
      "Epoch 192/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8848 - accuracy: 0.3000 - val_loss: 3.0218 - val_accuracy: 0.2737\n",
      "Epoch 193/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8857 - accuracy: 0.2995 - val_loss: 3.0218 - val_accuracy: 0.2743\n",
      "Epoch 194/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8848 - accuracy: 0.3003 - val_loss: 3.0219 - val_accuracy: 0.2737\n",
      "Epoch 195/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8840 - accuracy: 0.3008 - val_loss: 3.0216 - val_accuracy: 0.2738\n",
      "Epoch 196/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.2997 - val_loss: 3.0220 - val_accuracy: 0.2741\n",
      "Epoch 197/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8850 - accuracy: 0.3007 - val_loss: 3.0217 - val_accuracy: 0.2737\n",
      "Epoch 198/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8850 - accuracy: 0.2999 - val_loss: 3.0219 - val_accuracy: 0.2741\n",
      "Epoch 199/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8851 - accuracy: 0.3008 - val_loss: 3.0219 - val_accuracy: 0.2743\n",
      "Epoch 200/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8844 - accuracy: 0.3008 - val_loss: 3.0215 - val_accuracy: 0.2737\n",
      "Epoch 201/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8840 - accuracy: 0.3020 - val_loss: 3.0219 - val_accuracy: 0.2735\n",
      "Epoch 202/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8848 - accuracy: 0.3005 - val_loss: 3.0216 - val_accuracy: 0.2738\n",
      "Epoch 203/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8858 - accuracy: 0.3006 - val_loss: 3.0220 - val_accuracy: 0.2745\n",
      "Epoch 204/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8850 - accuracy: 0.2999 - val_loss: 3.0217 - val_accuracy: 0.2738\n",
      "Epoch 205/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8864 - accuracy: 0.3004 - val_loss: 3.0216 - val_accuracy: 0.2734\n",
      "Epoch 206/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8849 - accuracy: 0.3006 - val_loss: 3.0219 - val_accuracy: 0.2746\n",
      "Epoch 207/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8853 - accuracy: 0.3010 - val_loss: 3.0221 - val_accuracy: 0.2737\n",
      "Epoch 208/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8841 - accuracy: 0.3011 - val_loss: 3.0218 - val_accuracy: 0.2741\n",
      "Epoch 209/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8851 - accuracy: 0.3002 - val_loss: 3.0219 - val_accuracy: 0.2735\n",
      "Epoch 210/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.2995 - val_loss: 3.0218 - val_accuracy: 0.2740\n",
      "Epoch 211/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8849 - accuracy: 0.3003 - val_loss: 3.0217 - val_accuracy: 0.2737\n",
      "Epoch 212/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8863 - accuracy: 0.3005 - val_loss: 3.0221 - val_accuracy: 0.2740\n",
      "Epoch 213/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.3006 - val_loss: 3.0222 - val_accuracy: 0.2740\n",
      "Epoch 214/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8849 - accuracy: 0.2998 - val_loss: 3.0220 - val_accuracy: 0.2738\n",
      "Epoch 215/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8839 - accuracy: 0.3006 - val_loss: 3.0220 - val_accuracy: 0.2742\n",
      "Epoch 216/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8848 - accuracy: 0.3010 - val_loss: 3.0219 - val_accuracy: 0.2741\n",
      "Epoch 217/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8847 - accuracy: 0.2995 - val_loss: 3.0221 - val_accuracy: 0.2741\n",
      "Epoch 218/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8825 - accuracy: 0.3017 - val_loss: 3.0217 - val_accuracy: 0.2737\n",
      "Epoch 219/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8841 - accuracy: 0.3004 - val_loss: 3.0218 - val_accuracy: 0.2740\n",
      "Epoch 220/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8847 - accuracy: 0.3010 - val_loss: 3.0216 - val_accuracy: 0.2742\n",
      "Epoch 221/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8837 - accuracy: 0.3010 - val_loss: 3.0217 - val_accuracy: 0.2741\n",
      "Epoch 222/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8848 - accuracy: 0.3007 - val_loss: 3.0218 - val_accuracy: 0.2741\n",
      "Epoch 223/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8850 - accuracy: 0.3013 - val_loss: 3.0225 - val_accuracy: 0.2738\n",
      "Epoch 224/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8855 - accuracy: 0.3008 - val_loss: 3.0221 - val_accuracy: 0.2735\n",
      "Epoch 225/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8843 - accuracy: 0.3017 - val_loss: 3.0218 - val_accuracy: 0.2738\n",
      "Epoch 226/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8846 - accuracy: 0.3009 - val_loss: 3.0220 - val_accuracy: 0.2740\n",
      "Epoch 227/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8838 - accuracy: 0.3006 - val_loss: 3.0217 - val_accuracy: 0.2739\n",
      "Epoch 228/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8851 - accuracy: 0.3003 - val_loss: 3.0217 - val_accuracy: 0.2736\n",
      "Epoch 229/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.3003 - val_loss: 3.0217 - val_accuracy: 0.2740\n",
      "Epoch 230/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8840 - accuracy: 0.3028 - val_loss: 3.0218 - val_accuracy: 0.2744\n",
      "Epoch 231/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8840 - accuracy: 0.3014 - val_loss: 3.0223 - val_accuracy: 0.2741\n",
      "Epoch 232/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8852 - accuracy: 0.3000 - val_loss: 3.0220 - val_accuracy: 0.2740\n",
      "Epoch 233/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8838 - accuracy: 0.3004 - val_loss: 3.0216 - val_accuracy: 0.2738\n",
      "Epoch 234/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8860 - accuracy: 0.3010 - val_loss: 3.0221 - val_accuracy: 0.2742\n",
      "Epoch 235/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8843 - accuracy: 0.3001 - val_loss: 3.0218 - val_accuracy: 0.2739\n",
      "Epoch 236/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8849 - accuracy: 0.3007 - val_loss: 3.0221 - val_accuracy: 0.2741\n",
      "Epoch 237/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8867 - accuracy: 0.2992 - val_loss: 3.0217 - val_accuracy: 0.2737\n",
      "Epoch 238/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8846 - accuracy: 0.3004 - val_loss: 3.0217 - val_accuracy: 0.2741\n",
      "Epoch 239/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8839 - accuracy: 0.3006 - val_loss: 3.0218 - val_accuracy: 0.2735\n",
      "Epoch 240/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8848 - accuracy: 0.3003 - val_loss: 3.0216 - val_accuracy: 0.2733\n",
      "Epoch 241/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8853 - accuracy: 0.3010 - val_loss: 3.0218 - val_accuracy: 0.2737\n",
      "Epoch 242/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8871 - accuracy: 0.3011 - val_loss: 3.0217 - val_accuracy: 0.2739\n",
      "Epoch 243/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8847 - accuracy: 0.3006 - val_loss: 3.0218 - val_accuracy: 0.2745\n",
      "Epoch 244/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8857 - accuracy: 0.2998 - val_loss: 3.0219 - val_accuracy: 0.2734\n",
      "Epoch 245/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8852 - accuracy: 0.3017 - val_loss: 3.0216 - val_accuracy: 0.2736\n",
      "Epoch 246/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8833 - accuracy: 0.3019 - val_loss: 3.0220 - val_accuracy: 0.2738\n",
      "Epoch 247/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8860 - accuracy: 0.3003 - val_loss: 3.0220 - val_accuracy: 0.2736\n",
      "Epoch 248/256\n",
      "391/391 [==============================] - 2s 4ms/step - loss: 2.8864 - accuracy: 0.3016 - val_loss: 3.0222 - val_accuracy: 0.2735\n",
      "Epoch 249/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8839 - accuracy: 0.3005 - val_loss: 3.0217 - val_accuracy: 0.2740\n",
      "Epoch 250/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8855 - accuracy: 0.3014 - val_loss: 3.0218 - val_accuracy: 0.2739\n",
      "Epoch 251/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8857 - accuracy: 0.3018 - val_loss: 3.0218 - val_accuracy: 0.2740\n",
      "Epoch 252/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8842 - accuracy: 0.3005 - val_loss: 3.0216 - val_accuracy: 0.2742\n",
      "Epoch 253/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8841 - accuracy: 0.3014 - val_loss: 3.0221 - val_accuracy: 0.2739\n",
      "Epoch 254/256\n",
      "391/391 [==============================] - 1s 3ms/step - loss: 2.8850 - accuracy: 0.3003 - val_loss: 3.0219 - val_accuracy: 0.2737\n",
      "Epoch 255/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8855 - accuracy: 0.2996 - val_loss: 3.0221 - val_accuracy: 0.2740\n",
      "Epoch 256/256\n",
      "391/391 [==============================] - 1s 4ms/step - loss: 2.8851 - accuracy: 0.2987 - val_loss: 3.0220 - val_accuracy: 0.2738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc67d4dd4d0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# обучения модели\n",
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=\"{epoch:02d}-{val_accuracy:.2f}.hdf5\", save_best_only=True),\n",
    "        \n",
    "    ],\n",
    "    use_multiprocessing=True,\n",
    "    workers=8,\n",
    "    epochs=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-24T18:33:28.973275Z",
     "iopub.status.busy": "2023-01-24T18:33:28.972578Z",
     "iopub.status.idle": "2023-01-24T18:33:31.255310Z",
     "shell.execute_reply": "2023-01-24T18:33:31.254165Z",
     "shell.execute_reply.started": "2023-01-24T18:33:28.973236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/ (stored 0%)\n",
      "  adding: kaggle/working/20-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/59-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/122-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/41-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/05-0.23.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/11-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/31-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/03-0.21.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/57-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/25-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/195-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/37-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/53-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/02-0.21.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/20-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/254-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/89-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/05-0.22.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/146-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/125-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/242-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/90-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/192-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/05-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/45-0.29.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/19-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/13-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/60-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/101-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/20-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/02-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/58-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/76-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/06-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/224-0.29.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/237-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/__notebook_source__.ipynb (deflated 44%)\n",
      "  adding: kaggle/working/139-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/03-0.22.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/133-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/48-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/07-0.23.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/30-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/50-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/09-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/55-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/11-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/125-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/238-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/85-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/182-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/42-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/163-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/51-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/23-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/11-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/65-0.29.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/44-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/148-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.19.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/25-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/05-0.21.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/16-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/03-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/230-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/178-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/37-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/123-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/84-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/04-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/66-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/134-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/199-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/26-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/19-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/04-0.24.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/12-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/229-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/162-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/04-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/06-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/165-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/44-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/158-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/191-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/30-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/49-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/09-0.24.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/04-0.21.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/36-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/228-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/02-0.19.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/18-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/162-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/227-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/03-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/02-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/186-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/03-0.20.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.16.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/129-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.23.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/136-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/194-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/86-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/21-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/186-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/02-0.18.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/218-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/57-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/59-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/13-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/37-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/238-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/242-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.13.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/10-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/53-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/64-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/215-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/82-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/12-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/64-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/204-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/38-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/.virtual_documents/ (stored 0%)\n",
      "  adding: kaggle/working/10-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/159-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/04-0.22.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/106-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/35-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/08-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/23-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/219-0.29.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.14.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/126-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/218-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/61-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/my_work.zip (stored 0%)\n",
      "  adding: kaggle/working/121-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/90-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/16-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/29-0.29.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/155-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/22-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/02-0.20.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/27-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/217-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.20.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/14-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/02-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/16-0.24.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/05-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/197-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/226-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/178-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/13-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/04-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/03-0.24.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/252-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/74-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/62-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/38-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/02-0.23.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/07-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/82-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/69-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/253-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/68-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/10-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/40-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/06-0.23.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/20-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/61-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/29-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/226-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/65-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/130-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/10-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/255-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/28-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/131-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/78-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/15-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/69-0.29.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/75-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/81-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/93-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.12.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/06-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/08-0.24.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/11-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/07-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/65-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/06-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/36-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/101-0.29.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/100-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/54-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/104-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/113-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/149-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/151-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/22-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/01-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/100-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/60-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/32-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/94-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/143-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/33-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/232-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/17-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/10-0.29.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/39-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/43-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/84-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/06-0.22.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/49-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/24-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/35-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/08-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/133-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/248-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/05-0.24.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/58-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/11-0.24.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/213-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/15-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/33-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/54-0.26.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/62-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/14-0.25.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/205-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/05-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/44-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/172-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/86-0.27.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/09-0.25.hdf5 (deflated 34%)\n",
      "  adding: kaggle/working/27-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/10-0.24.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/245-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/88-0.28.hdf5 (deflated 33%)\n",
      "  adding: kaggle/working/07-0.24.hdf5 (deflated 33%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r file.zip /kaggle/working/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
